{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23a593d",
   "metadata": {},
   "source": [
    "# Loading Movie Lens dataset into RDDs\n",
    "\n",
    "- Collaborative filtering is a technique for recommender systems wherein users' ratings and interactions with various products are used to recommend new ones. With the advent of Machine Learning and parallelized processing of data, Recommender systems have become widely popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags. In this 3-part exercise, your goal is to develop a simple movie recommendation system using PySpark MLlib using a subset of [MovieLens 100k dataset](https://grouplens.org/datasets/movielens/100k/).\n",
    "\n",
    "- In the first part, you'll first load the MovieLens data (`ratings.csv`) into RDD and from each line in the RDD which is formatted as `userId`,`movieId`,`rating`,`timestamp`, you'll need to map the MovieLens data to a Ratings object (`userID`, `productID`, `rating`) after removing timestamp column and finally you'll split the RDD into training and test RDDs.\n",
    "\n",
    "- Remember, you have a `SparkContext` `sc` available in your workspace. Also `file_path` variable (which is the path to the `ratings.csv` file), and `ALS` class are already available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef3015",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Load the `ratings.csv` dataset into an RDD.\n",
    "- Split the RDD using `,` as a delimiter.\n",
    "- For each line of the RDD, using `Rating()` class create a tuple of `userID`, `productID`, `rating`.\n",
    "- Randomly split the data into training data and test data (0.8 and 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'file://<pwd>/Dataset/ratings.csv'\n",
    "\n",
    "# Load the data into RDD\n",
    "data = sc.____(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.____(lambda l: l.split('____'))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.____(lambda line: Rating(int(line[0]), int(____), float(____)))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.____([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fcc0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n",
      "80072\n",
      "19932\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "\n",
    "file_path = 'file:///home/talentum/test-jupyter/P2/M4/SM2/Dataset/ratings.csv'\n",
    "\n",
    "# Load the data into RDD\n",
    "data = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "print(ratings_final.count())\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])\n",
    "# print(type(training_data))\n",
    "# print(type(test_data))\n",
    "# print(training_data.getNumPartitions())\n",
    "print(training_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab1b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.19.4\n",
      "scipy 1.5.4\n",
      "pandas 1.1.4\n",
      "matplotlib 3.3.3\n"
     ]
    }
   ],
   "source": [
    "# This is just a Test Note created by Amit\n",
    "import numpy\n",
    "import scipy\n",
    "import pandas\n",
    "import matplotlib\n",
    "\n",
    "print(\"numpy \" + numpy.__version__)\n",
    "print(\"scipy \" + scipy.__version__)\n",
    "print(\"pandas \" + pandas.__version__)\n",
    "print(\"matplotlib \" + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee454532",
   "metadata": {},
   "source": [
    "> Good job with preprocessing the data. It's time to train the ALS model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

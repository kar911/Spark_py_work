{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using user defined functions in Spark\n",
    "\n",
    "- You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "- For this exercise, we'll use our `voter_df` DataFrame, but you're going to replace the `first_name` column with the first and middle names.\n",
    "\n",
    "- The `pyspark.sql.functions` library is available under the alias `F`. The classes from `pyspark.sql.types` are already imported.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Edit the `getFirstAndMiddle()` function to return a space separated string of names, except the last entry in the names list.\n",
    "- Define the function as a user-defined function. It should return a string type.\n",
    "- Create a new column on `voter_df` called `first_and_middle_name` using your UDF.\n",
    "- Show the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/talentum/spark-jupyter/3_UserDefinedFunctions\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|first_and_middle_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|    Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|   Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|  Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|         Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|         Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|  Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|     Scott|   Griggs|         Scott Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|        B.|  McGough|      B. Adam McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|       Lee| Kleinman|         Lee Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|        Sandy Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|    Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|   Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|  Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|         Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|         Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|   Carolyn|   Arnold|  Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|    Rickey| Callahan|   Rickey D. Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|    Jennifer S. Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|     Sandy|  Greyson|        Sandy Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|    Jennifer S. Gates|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "voter_df = spark.read.format('csv').options(Header=True).load('file:///home/talentum/spark-jupyter/3_UserDefinedFunctions/Dataset/DallasCouncilVoters.csv.gz')\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "voter_df=voter_df.filter(voter_df.VOTER_NAME.isNotNull())\n",
    "\n",
    "\n",
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names \n",
    "  return ' '.join(names[])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "b=voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, splits: array<string>, first_name: string, last_name: string, first_and_middle_name: string]\n"
     ]
    }
   ],
   "source": [
    "print(voter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(first_name='Jennifer', splits=['Jennifer', 'S.', 'Gates']),\n",
       " Row(first_name='Philip', splits=['Philip', 'T.', 'Kingston']),\n",
       " Row(first_name='Michael', splits=['Michael', 'S.', 'Rawlings']),\n",
       " Row(first_name='Adam', splits=['Adam', 'Medrano']),\n",
       " Row(first_name='Casey', splits=['Casey', 'Thomas'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.select('first_name','splits').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_name  middle_name'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=['first_name','middle_name','last_name']\n",
    "\"  \".join(list1[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-3461db6d1898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvoter_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoter_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoter_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVOTER_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvoter_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "voter_df=voter_df.filter(voter_df.VOTER_NAME.isNull())\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+----------+----------+---------------------+\n",
      "|summary|                DATE|               TITLE|          VOTER_NAME|first_name| last_name|first_and_middle_name|\n",
      "+-------+--------------------+--------------------+--------------------+----------+----------+---------------------+\n",
      "|  count|               44122|               44122|               44122|     44122|     44122|                44122|\n",
      "|   mean|                null|                null|                null|      null|      null|                 null|\n",
      "| stddev|                null|                null|                null|      null|      null|                 null|\n",
      "|    min|          01/03/2018|   authorize   an...|   the   final  2...|          |011018__42|  the final 2018 A...|\n",
      "|    max|MADELEINE JOHNSON...|       Mayor Pro Tem|   Tiffinni A. Young|  Tiffinni|     rooms|    Tiffinni A. Young|\n",
      "+-------+--------------------+--------------------+--------------------+----------+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----------+---------+---------------------+\n",
      "|DATE|TITLE|VOTER_NAME|splits|first_name|last_name|first_and_middle_name|\n",
      "+----+-----+----------+------+----------+---------+---------------------+\n",
      "+----+-----+----------+------+----------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v=voter_df.filter(voter_df[\"VOTER_NAME\"].isNull())\n",
    "v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Julia|     |  null|\n",
      "|  Ram| null| 200.0|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data=[\n",
    "    ('James','CA',np.NaN), ('Julia','',None),\n",
    "    ('Ram',None,200.0),('Ramya','NULL',np.NAN)\n",
    "\n",
    "]\n",
    "df=spark.createDataFrame(data,['name','state','number'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "| Ram| null| 200.0|\n",
      "+----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|Julia|     |  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.isNull()).show()\n",
    "df.filter(df.number.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.isin([np.NAN,None])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.contains('Null') | df.state.contains('null') | df.state.contains('NULL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.contains('Null') | df.number.contains('null') | df.number.contains('NULL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Julia|     |  null|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.contains('')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|  Ram| null| 200.0|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.contains('')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.isnan(df.number)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|Julia|     |  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.isnull(df.number)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.state.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.number.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|number|\n",
      "+-----+------+\n",
      "|James|   NaN|\n",
      "|Julia|  null|\n",
      "|  Ram| 200.0|\n",
      "|Ramya|   NaN|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['name','number']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['name','number']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(number)|\n",
      "+-------------+\n",
      "|            3|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('count(number)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+------+\n",
      "|summary| name|state|number|\n",
      "+-------+-----+-----+------+\n",
      "|  count|    4|    3|     3|\n",
      "|   mean| null| null|   NaN|\n",
      "| stddev| null| null|   NaN|\n",
      "|    min|James|     | 200.0|\n",
      "|    max|Ramya| NULL|   NaN|\n",
      "+-------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+---+\n",
      "|summary| name|       kk| ll|\n",
      "+-------+-----+---------+---+\n",
      "|  count|    4|        3|  3|\n",
      "|   mean| null|     null|NaN|\n",
      "| stddev| null|     null|NaN|\n",
      "|    min|James|  JamesCA|0.0|\n",
      "|    max|Ramya|RamyaNULL|NaN|\n",
      "+-------+-----+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(['name','concat(name,state)' 'kk','number - abs(number)' 'll']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------+\n",
      "|null_in_name|null_in_state|null_in_number|\n",
      "+------------+-------------+--------------+\n",
      "|           0|            1|             3|\n",
      "+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(['count(*) - count(distinct(name))' 'null_in_name'\n",
    "               ,'count(*) - count(distinct(state))' 'null_in_state'\n",
    "               ,'count(*)-count(nullif(number,\"NaN\"))' 'null_in_number']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   ll|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "|200.0|\n",
      "| null|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(['nullif(number,\"NaN\") ' 'll']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   ll|\n",
      "+-----+\n",
      "|false|\n",
      "| true|\n",
      "|false|\n",
      "|false|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(['isnull(nullif(number,\"np.NaN\")) ' 'll']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "|   4|    3|     3|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*[F.count(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Julia|     |  null|\n",
      "|  Ram| null| 200.0|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`namen`' given input columns: [name, state, number];;\\n'Project ['namen]\\n+- LogicalRDD [name#3156, state#3157, number#3158], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1856.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`namen`' given input columns: [name, state, number];;\n'Project ['namen]\n+- LogicalRDD [name#3156, state#3157, number#3158], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-afd11c6160ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`namen`' given input columns: [name, state, number];;\\n'Project ['namen]\\n+- LogicalRDD [name#3156, state#3157, number#3158], false\\n\""
     ]
    }
   ],
   "source": [
    "df.select(['name' 'n']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "|   0|    1|     3|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "+----+-----+------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "condition should be string or Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-6401aecde386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: condition should be string or Column"
     ]
    }
   ],
   "source": [
    "df.filter(F.isnan(df.state)).show()\n",
    "data_count=[\n",
    "    (df.filter(df.name.isNull() | F.isnan(df.name) | (df.name == '')).count(),\n",
    "     df.filter(df.state.isNull() | F.isnan(df.state) | (df.state == '')).count(),\n",
    "     df.filter(df.number.isNull() | F.isnan(df.number) | (df.number == '')).count()\n",
    "    )\n",
    "]\n",
    "\n",
    "df.filter(data_count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.isnan(df.state)).show()\n",
    "data_count=[\n",
    "    (df.filter(df.name.isNull() | F.isnan(df.name)).count(),\n",
    "     df.filter(df.state.isNull() | F.isnan(df.state)).count(),\n",
    "     df.filter(df.number.isNull() | F.isnan(df.number)).count()\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- number: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.contains('NaN')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|  Ram| null| 200.0|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.contains('')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|number|\n",
      "+----+-----+------+\n",
      "| Ram| null| 200.0|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.contains('200')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|number|\n",
      "+-----+-----+------+\n",
      "|James|   CA|   NaN|\n",
      "|Ramya| NULL|   NaN|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.number.contains('NaN')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
